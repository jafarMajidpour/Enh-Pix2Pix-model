{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eafd29-0d05-4d0c-a6b8-4ef73ce830fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile, ImageDraw, ImageFont\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"scikit-image\"])\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.switch_backend('Agg')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedLoss(nn.Module):\n",
    "   \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def generator_loss(self, generated, target, disc_output):\n",
    "        # L1 reconstruction loss\n",
    "        l1 = self.l1_loss(generated, target)\n",
    "        \n",
    "        # Adversarial loss\n",
    "        adv = self.mse_loss(disc_output, torch.ones_like(disc_output))\n",
    "        \n",
    "        # Total loss\n",
    "        total = self.config.lambda_l1 * l1 + self.config.lambda_adversarial * adv\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'l1': l1,\n",
    "            'adversarial': adv\n",
    "        }\n",
    "    \n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_loss = self.mse_loss(real_output, torch.ones_like(real_output))\n",
    "        fake_loss = self.mse_loss(fake_output, torch.zeros_like(fake_output))\n",
    "        return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "class EnhancedTrainer:\n",
    "    \"\"\"Enhanced trainer with validation support\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        # Set random seeds\n",
    "        torch.manual_seed(config.random_seed)\n",
    "        np.random.seed(config.random_seed)\n",
    "        random.seed(config.random_seed)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.generator = EnhancedGenerator(\n",
    "            config.input_nc, config.output_nc, config.ngf\n",
    "        ).to(config.device)\n",
    "        \n",
    "        self.discriminator = EnhancedDiscriminator(\n",
    "            config.input_nc + config.output_nc, config.ndf\n",
    "        ).to(config.device)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.g_optimizer = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=config.learning_rate_g,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        \n",
    "        self.d_optimizer = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=config.learning_rate_d,\n",
    "            betas=(config.beta1, config.beta2)\n",
    "        )\n",
    "        \n",
    "        # Initialize loss and metrics\n",
    "        self.criterion = EnhancedLoss(config)\n",
    "        self.metrics = EnhancedMetrics(config.device)\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_g_loss': [],\n",
    "            'train_d_loss': [],\n",
    "            'train_psnr': [],\n",
    "            'train_mae': [],\n",
    "            'val_g_loss': [],\n",
    "            'val_d_loss': [],\n",
    "            'val_psnr': [],\n",
    "            'val_mae': [],\n",
    "            'val_ssim': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_psnr = 0\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "        logger.info(\"Enhanced trainer initialized successfully\")\n",
    "    \n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        epoch_losses = {'g_loss': 0, 'd_loss': 0, 'psnr': 0, 'mae': 0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                input_img = batch['input'].to(self.config.device)\n",
    "                target_img = batch['target'].to(self.config.device)\n",
    "                \n",
    "                # Train Discriminator\n",
    "                self.d_optimizer.zero_grad()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fake_img = self.generator(input_img)\n",
    "                \n",
    "                real_pred = self.discriminator(input_img, target_img)\n",
    "                fake_pred = self.discriminator(input_img, fake_img.detach())\n",
    "                \n",
    "                d_loss = self.criterion.discriminator_loss(real_pred, fake_pred)\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "                \n",
    "                # Train Generator\n",
    "                self.g_optimizer.zero_grad()\n",
    "                \n",
    "                fake_img = self.generator(input_img)\n",
    "                fake_pred = self.discriminator(input_img, fake_img)\n",
    "                \n",
    "                g_losses = self.criterion.generator_loss(fake_img, target_img, fake_pred)\n",
    "                g_loss = g_losses['total']\n",
    "                \n",
    "                g_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 1.0)\n",
    "                self.g_optimizer.step()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                with torch.no_grad():\n",
    "                    psnr = self.metrics.calculate_psnr(fake_img, target_img).item()\n",
    "                    mae = F.l1_loss(fake_img, target_img).item()\n",
    "                \n",
    "                # Update epoch totals\n",
    "                epoch_losses['g_loss'] += g_loss.item()\n",
    "                epoch_losses['d_loss'] += d_loss.item()\n",
    "                epoch_losses['psnr'] += psnr\n",
    "                epoch_losses['mae'] += mae\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'G_Loss': f'{g_loss.item():.4f}',\n",
    "                    'D_Loss': f'{d_loss.item():.4f}',\n",
    "                    'PSNR': f'{psnr:.2f}'\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if num_batches == 0:\n",
    "            raise RuntimeError(\"No valid batches processed!\")\n",
    "        \n",
    "        # Calculate averages\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] /= num_batches\n",
    "        \n",
    "        return epoch_losses\n",
    "    \n",
    "    def validate_epoch(self, val_loader, epoch):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n",
    "        \n",
    "        val_losses = {'g_loss': 0, 'd_loss': 0, 'psnr': 0, 'mae': 0, 'ssim': 0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch}')\n",
    "            \n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                if batch is None:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    input_img = batch['input'].to(self.config.device)\n",
    "                    target_img = batch['target'].to(self.config.device)\n",
    "                    \n",
    "                    # Generate fake image\n",
    "                    fake_img = self.generator(input_img)\n",
    "                    \n",
    "                    # Calculate discriminator loss\n",
    "                    real_pred = self.discriminator(input_img, target_img)\n",
    "                    fake_pred = self.discriminator(input_img, fake_img)\n",
    "                    d_loss = self.criterion.discriminator_loss(real_pred, fake_pred)\n",
    "                    \n",
    "                    # Calculate generator loss\n",
    "                    g_losses = self.criterion.generator_loss(fake_img, target_img, fake_pred)\n",
    "                    g_loss = g_losses['total']\n",
    "                    \n",
    "                    # Calculate comprehensive metrics\n",
    "                    metrics = self.metrics.evaluate_comprehensive(fake_img, target_img)\n",
    "                    \n",
    "                    # Update totals\n",
    "                    val_losses['g_loss'] += g_loss.item()\n",
    "                    val_losses['d_loss'] += d_loss.item()\n",
    "                    val_losses['psnr'] += metrics['psnr']\n",
    "                    val_losses['mae'] += metrics['mae']\n",
    "                    val_losses['ssim'] += metrics['ssim']\n",
    "                    num_batches += 1\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({\n",
    "                        'Val_PSNR': f'{metrics[\"psnr\"]:.2f}',\n",
    "                        'Val_SSIM': f'{metrics[\"ssim\"]:.3f}'\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if num_batches == 0:\n",
    "            logger.warning(\"No valid validation batches processed!\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate averages\n",
    "        for key in val_losses:\n",
    "            val_losses[key] /= num_batches\n",
    "        \n",
    "        return val_losses\n",
    "    \n",
    "    def update_history(self, train_results, val_results):\n",
    "        \"\"\"Update training history\"\"\"\n",
    "        # Training metrics\n",
    "        self.history['train_g_loss'].append(train_results['g_loss'])\n",
    "        self.history['train_d_loss'].append(train_results['d_loss'])\n",
    "        self.history['train_psnr'].append(train_results['psnr'])\n",
    "        self.history['train_mae'].append(train_results['mae'])\n",
    "        \n",
    "        # Validation metrics\n",
    "        if val_results:\n",
    "            self.history['val_g_loss'].append(val_results['g_loss'])\n",
    "            self.history['val_d_loss'].append(val_results['d_loss'])\n",
    "            self.history['val_psnr'].append(val_results['psnr'])\n",
    "            self.history['val_mae'].append(val_results['mae'])\n",
    "            self.history['val_ssim'].append(val_results['ssim'])\n",
    "        else:\n",
    "            # Append None or last value if validation failed\n",
    "            for key in ['val_g_loss', 'val_d_loss', 'val_psnr', 'val_mae', 'val_ssim']:\n",
    "                if self.history[key]:\n",
    "                    self.history[key].append(self.history[key][-1])\n",
    "                else:\n",
    "                    self.history[key].append(0)\n",
    "    \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': self.generator.state_dict(),\n",
    "                'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "                'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n",
    "                'd_optimizer_state_dict': self.d_optimizer.state_dict(),\n",
    "                'history': self.history,\n",
    "                'best_val_psnr': self.best_val_psnr,\n",
    "                'best_epoch': self.best_epoch\n",
    "            }\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = os.path.join(self.config.experiment_dir, 'checkpoints', f'checkpoint_epoch_{epoch}.pth')\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            \n",
    "            if is_best:\n",
    "                best_path = os.path.join(self.config.experiment_dir, 'checkpoints', 'best_model.pth')\n",
    "                torch.save(checkpoint, best_path)\n",
    "                logger.info(f\"💾 Best model saved at epoch {epoch} (PSNR: {self.best_val_psnr:.2f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving checkpoint: {e}\")\n",
    "    \n",
    "    def generate_samples(self, dataloader, epoch, num_samples=8, split_name='train'):\n",
    "        \"\"\"Generate sample images\"\"\"\n",
    "        self.generator.eval()\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                batch = next(iter(dataloader))\n",
    "                if batch is None:\n",
    "                    return\n",
    "                \n",
    "                input_img = batch['input'][:num_samples].to(self.config.device)\n",
    "                target_img = batch['target'][:num_samples].to(self.config.device)\n",
    "                \n",
    "                generated_img = self.generator(input_img)\n",
    "                \n",
    "                # Save comparison images\n",
    "                for i in range(min(num_samples, input_img.size(0))):\n",
    "                    # Convert to [0, 1] range\n",
    "                    input_np = (input_img[i].cpu() + 1) / 2\n",
    "                    generated_np = (generated_img[i].cpu() + 1) / 2\n",
    "                    target_np = (target_img[i].cpu() + 1) / 2\n",
    "                    \n",
    "                    # Create side-by-side comparison\n",
    "                    comparison = torch.cat([input_np, generated_np, target_np], dim=2)\n",
    "                    \n",
    "                    # Save image\n",
    "                    save_path = os.path.join(\n",
    "                        self.config.experiment_dir, \n",
    "                        'generated_samples', \n",
    "                        f'{split_name}_epoch_{epoch}_sample_{i}.png'\n",
    "                    )\n",
    "                    transforms.ToPILImage()(comparison).save(save_path)\n",
    "                \n",
    "                logger.info(f\"📸 {split_name.capitalize()} sample images saved for epoch {epoch}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error generating {split_name} samples: {e}\")\n",
    "        \n",
    "        self.generator.train()\n",
    "    \n",
    "    def plot_training_progress(self):\n",
    "        \"\"\"Plot comprehensive training progress\"\"\"\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "            epochs = range(1, len(self.history['train_g_loss']) + 1)\n",
    "            \n",
    "            # Loss curves\n",
    "            axes[0, 0].plot(epochs, self.history['train_g_loss'], 'b-', label='Train Generator', alpha=0.7)\n",
    "            axes[0, 0].plot(epochs, self.history['val_g_loss'], 'r-', label='Val Generator', alpha=0.7)\n",
    "            axes[0, 0].set_title('Generator Loss')\n",
    "            axes[0, 0].set_xlabel('Epoch')\n",
    "            axes[0, 0].set_ylabel('Loss')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[0, 1].plot(epochs, self.history['train_d_loss'], 'b-', label='Train Discriminator', alpha=0.7)\n",
    "            axes[0, 1].plot(epochs, self.history['val_d_loss'], 'r-', label='Val Discriminator', alpha=0.7)\n",
    "            axes[0, 1].set_title('Discriminator Loss')\n",
    "            axes[0, 1].set_xlabel('Epoch')\n",
    "            axes[0, 1].set_ylabel('Loss')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # PSNR\n",
    "            axes[0, 2].plot(epochs, self.history['train_psnr'], 'b-', label='Train PSNR', alpha=0.7)\n",
    "            axes[0, 2].plot(epochs, self.history['val_psnr'], 'r-', label='Val PSNR', alpha=0.7)\n",
    "            axes[0, 2].set_title('PSNR Over Time')\n",
    "            axes[0, 2].set_xlabel('Epoch')\n",
    "            axes[0, 2].set_ylabel('PSNR (dB)')\n",
    "            axes[0, 2].legend()\n",
    "            axes[0, 2].grid(True, alpha=0.3)\n",
    "            \n",
    "            # MAE\n",
    "            axes[1, 0].plot(epochs, self.history['train_mae'], 'b-', label='Train MAE', alpha=0.7)\n",
    "            axes[1, 0].plot(epochs, self.history['val_mae'], 'r-', label='Val MAE', alpha=0.7)\n",
    "            axes[1, 0].set_title('MAE Over Time')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('MAE')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # SSIM\n",
    "            axes[1, 1].plot(epochs, self.history['val_ssim'], 'g-', label='Val SSIM', alpha=0.7)\n",
    "            axes[1, 1].set_title('SSIM Over Time')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('SSIM')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Summary statistics\n",
    "            if epochs:\n",
    "                final_train_psnr = self.history['train_psnr'][-1] if self.history['train_psnr'] else 0\n",
    "                final_val_psnr = self.history['val_psnr'][-1] if self.history['val_psnr'] else 0\n",
    "                final_val_ssim = self.history['val_ssim'][-1] if self.history['val_ssim'] else 0\n",
    "                \n",
    "                axes[1, 2].text(0.1, 0.8, f'Best Val PSNR: {self.best_val_psnr:.2f} dB (Epoch {self.best_epoch})', \n",
    "                               transform=axes[1, 2].transAxes, fontsize=12, weight='bold')\n",
    "                axes[1, 2].text(0.1, 0.7, f'Final Train PSNR: {final_train_psnr:.2f} dB', \n",
    "                               transform=axes[1, 2].transAxes, fontsize=10)\n",
    "                axes[1, 2].text(0.1, 0.6, f'Final Val PSNR: {final_val_psnr:.2f} dB', \n",
    "                               transform=axes[1, 2].transAxes, fontsize=10)\n",
    "                axes[1, 2].text(0.1, 0.5, f'Final Val SSIM: {final_val_ssim:.3f}', \n",
    "                               transform=axes[1, 2].transAxes, fontsize=10)\n",
    "                axes[1, 2].text(0.1, 0.4, f'Total Epochs: {len(epochs)}', \n",
    "                               transform=axes[1, 2].transAxes, fontsize=10)\n",
    "            \n",
    "            axes[1, 2].set_title('Training Summary')\n",
    "            axes[1, 2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            progress_path = os.path.join(self.config.experiment_dir, 'training_progress', 'training_curves.png')\n",
    "            plt.savefig(progress_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            logger.info(f\" Training progress plot saved\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting progress: {e}\")\n",
    "\n",
    "class TestEvaluator:\n",
    "    \"\"\"Comprehensive test evaluation with detailed analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, config, model_path):\n",
    "        self.config = config\n",
    "        self.model_path = model_path\n",
    "        self.device = config.device\n",
    "        \n",
    "        # Load trained model\n",
    "        self.generator = self._load_model()\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.metrics_calculator = EnhancedMetrics(self.device)\n",
    "        \n",
    "        # Results storage\n",
    "        self.test_results = {\n",
    "            'metrics': [],\n",
    "            'filenames': [],\n",
    "            'individual_results': []\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Test evaluator initialized\")\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the trained generator model\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(self.model_path, map_location=self.device)\n",
    "            generator = EnhancedGenerator(\n",
    "                self.config.input_nc, \n",
    "                self.config.output_nc, \n",
    "                self.config.ngf\n",
    "            ).to(self.device)\n",
    "            generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "            generator.eval()\n",
    "            logger.info(f\" Model loaded from {self.model_path}\")\n",
    "            return generator\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Failed to load model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate_test_dataset(self, test_loader):\n",
    "        \"\"\"Evaluate test dataset comprehensively\"\"\"\n",
    "        \n",
    "        print(\"🧪 \" + \"=\"*60)\n",
    "        print(\"🧪 COMPREHENSIVE TEST EVALUATION\")\n",
    "        print(\"🧪 \" + \"=\"*60)\n",
    "        \n",
    "        # Create output directories\n",
    "        test_output_dir = os.path.join(self.config.experiment_dir, 'test_results')\n",
    "        self._create_output_directories(test_output_dir)\n",
    "        \n",
    "        # Process test images\n",
    "        self._process_test_dataset(test_loader, test_output_dir)\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        aggregate_metrics = self._calculate_aggregate_metrics()\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self._generate_comprehensive_visualizations(test_output_dir, aggregate_metrics)\n",
    "        \n",
    "        # Save detailed results\n",
    "        self._save_comprehensive_results(test_output_dir, aggregate_metrics)\n",
    "        \n",
    "        print(f\"\\n🎉 Test evaluation completed!\")\n",
    "        print(f\"📁 Results saved to: {test_output_dir}\")\n",
    "        \n",
    "        return aggregate_metrics\n",
    "    \n",
    "    def _create_output_directories(self, base_dir):\n",
    "        \"\"\"Create output directories for test results\"\"\"\n",
    "        subdirs = [\n",
    "            'generated_images', 'comparisons', 'error_maps',\n",
    "            'metrics_analysis', 'quality_assessment', 'detailed_reports'\n",
    "        ]\n",
    "        \n",
    "        for subdir in subdirs:\n",
    "            os.makedirs(os.path.join(base_dir, subdir), exist_ok=True)\n",
    "    \n",
    "    def _process_test_dataset(self, test_loader, output_dir):\n",
    "        \"\"\"Process test dataset and generate comprehensive results\"\"\"\n",
    "        \n",
    "        print(f\"\\n📊 Processing test dataset...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Processing test batches\")):\n",
    "                if batch is None:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    input_imgs = batch['input'].to(self.config.device)\n",
    "                    target_imgs = batch['target'].to(self.config.device)\n",
    "                    filenames = batch['filename']\n",
    "                    \n",
    "                    # Generate outputs\n",
    "                    generated_imgs = self.generator(input_imgs)\n",
    "                    \n",
    "                    # Process each image in the batch\n",
    "                    for i in range(input_imgs.size(0)):\n",
    "                        self._process_single_test_image(\n",
    "                            input_imgs[i:i+1], generated_imgs[i:i+1], target_imgs[i:i+1],\n",
    "                            filenames[i], output_dir, batch_idx * self.config.batch_size + i\n",
    "                        )\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\" Error processing batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    def _process_single_test_image(self, input_img, generated_img, target_img, \n",
    "                                 filename, output_dir, img_idx):\n",
    "        \"\"\"Process a single test image comprehensively\"\"\"\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        metrics = self.metrics_calculator.evaluate_comprehensive(generated_img, target_img)\n",
    "        \n",
    "        # Store results\n",
    "        self.test_results['metrics'].append(metrics)\n",
    "        self.test_results['filenames'].append(filename)\n",
    "        self.test_results['individual_results'].append({\n",
    "            'filename': filename,\n",
    "            'index': img_idx,\n",
    "            'metrics': metrics\n",
    "        })\n",
    "        \n",
    "        # Save visual results\n",
    "        self._save_visual_results(input_img, generated_img, target_img, filename, output_dir, metrics)\n",
    "        \n",
    "        # Create error map\n",
    "        self._create_detailed_error_map(generated_img, target_img, filename, output_dir)\n",
    "    \n",
    "    def _save_visual_results(self, input_img, generated_img, target_img, filename, output_dir, metrics):\n",
    "        \"\"\"Save visual comparison results\"\"\"\n",
    "        \n",
    "        # Convert tensors to PIL images\n",
    "        input_pil = self._tensor_to_pil(input_img.squeeze(0))\n",
    "        generated_pil = self._tensor_to_pil(generated_img.squeeze(0))\n",
    "        target_pil = self._tensor_to_pil(target_img.squeeze(0))\n",
    "        \n",
    "        # Save individual generated image\n",
    "        generated_path = os.path.join(output_dir, 'generated_images', f'{filename}_generated.png')\n",
    "        generated_pil.save(generated_path)\n",
    "        \n",
    "        # Create detailed comparison with metrics\n",
    "        comparison_width = input_pil.width * 3\n",
    "        comparison_height = input_pil.height + 80  # Extra space for text\n",
    "        comparison = Image.new('RGB', (comparison_width, comparison_height), (255, 255, 255))\n",
    "        \n",
    "        # Paste images\n",
    "        comparison.paste(input_pil, (0, 80))\n",
    "        comparison.paste(generated_pil, (input_pil.width, 80))\n",
    "        comparison.paste(target_pil, (input_pil.width * 2, 80))\n",
    "        \n",
    "        # Add labels and metrics\n",
    "        draw = ImageDraw.Draw(comparison)\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "            small_font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "            small_font = font\n",
    "        \n",
    "        # Labels\n",
    "        draw.text((10, 10), \"Input\", fill=(0, 0, 0), font=font)\n",
    "        draw.text((input_pil.width + 10, 10), \"Generated\", fill=(0, 0, 0), font=font)\n",
    "        draw.text((input_pil.width * 2 + 10, 10), \"Target\", fill=(0, 0, 0), font=font)\n",
    "        \n",
    "        # Metrics\n",
    "        metrics_text = f\"PSNR: {metrics['psnr']:.2f}dB | SSIM: {metrics['ssim']:.3f} | MAE: {metrics['mae']:.4f}\"\n",
    "        draw.text((10, 40), metrics_text, fill=(0, 0, 0), font=small_font)\n",
    "        \n",
    "        # Quality assessment\n",
    "        quality = self._assess_quality(metrics['psnr'])\n",
    "        quality_color = self._get_quality_color(quality)\n",
    "        draw.text((10, 60), f\"Quality: {quality}\", fill=quality_color, font=small_font)\n",
    "        \n",
    "        # Save comparison\n",
    "        comparison_path = os.path.join(output_dir, 'comparisons', f'{filename}_detailed_comparison.png')\n",
    "        comparison.save(comparison_path)\n",
    "    \n",
    "    def _tensor_to_pil(self, tensor):\n",
    "        \"\"\"Convert tensor to PIL image\"\"\"\n",
    "        tensor = (tensor + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "        tensor = torch.clamp(tensor, 0, 1)\n",
    "        return transforms.ToPILImage()(tensor)\n",
    "    \n",
    "    def _assess_quality(self, psnr):\n",
    "        \"\"\"Assess image quality based on PSNR\"\"\"\n",
    "        if psnr >= 35:\n",
    "            return \"Excellent\"\n",
    "        elif psnr >= 30:\n",
    "            return \"Very Good\"\n",
    "        elif psnr >= 25:\n",
    "            return \"Good\"\n",
    "        elif psnr >= 20:\n",
    "            return \"Fair\"\n",
    "        else:\n",
    "            return \"Poor\"\n",
    "    \n",
    "    def _get_quality_color(self, quality):\n",
    "        \"\"\"Get color for quality assessment\"\"\"\n",
    "        colors = {\n",
    "            \"Excellent\": (0, 128, 0),\n",
    "            \"Very Good\": (50, 205, 50),\n",
    "            \"Good\": (255, 165, 0),\n",
    "            \"Fair\": (255, 140, 0),\n",
    "            \"Poor\": (255, 0, 0)\n",
    "        }\n",
    "        return colors.get(quality, (0, 0, 0))\n",
    "    \n",
    "    def _create_detailed_error_map(self, generated, target, filename, output_dir):\n",
    "        \"\"\"Create detailed error visualization\"\"\"\n",
    "        # Calculate pixel-wise error\n",
    "        error = torch.abs(generated - target)\n",
    "        error_mean = torch.mean(error, dim=1, keepdim=True)\n",
    "        error_np = error_mean.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        # Create comprehensive error visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Error heatmap\n",
    "        im1 = axes[0, 0].imshow(error_np, cmap='hot', interpolation='nearest')\n",
    "        axes[0, 0].set_title('Pixel-wise Error (MAE)')\n",
    "        axes[0, 0].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[0, 0])\n",
    "        \n",
    "        # Error histogram\n",
    "        axes[0, 1].hist(error_np.flatten(), bins=50, alpha=0.7, color='red')\n",
    "        axes[0, 1].set_title('Error Distribution')\n",
    "        axes[0, 1].set_xlabel('Error Value')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Thresholded error (high error regions)\n",
    "        threshold = np.percentile(error_np, 90)\n",
    "        high_error = error_np > threshold\n",
    "        axes[1, 0].imshow(high_error, cmap='Reds', interpolation='nearest')\n",
    "        axes[1, 0].set_title(f'High Error Regions (>{threshold:.3f})')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # Error statistics\n",
    "        error_stats = {\n",
    "            'Mean Error': np.mean(error_np),\n",
    "            'Std Error': np.std(error_np),\n",
    "            'Max Error': np.max(error_np),\n",
    "            'Min Error': np.min(error_np),\n",
    "            '95th Percentile': np.percentile(error_np, 95)\n",
    "        }\n",
    "        \n",
    "        stats_text = '\\n'.join([f'{k}: {v:.4f}' for k, v in error_stats.items()])\n",
    "        axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, \n",
    "                        fontsize=12, verticalalignment='center')\n",
    "        axes[1, 1].set_title('Error Statistics')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Error Analysis: {filename}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        error_path = os.path.join(output_dir, 'error_maps', f'{filename}_error_analysis.png')\n",
    "        plt.savefig(error_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _calculate_aggregate_metrics(self):\n",
    "        \"\"\"Calculate comprehensive aggregate metrics\"\"\"\n",
    "        if not self.test_results['metrics']:\n",
    "            return {}\n",
    "        \n",
    "        # Collect all metric values\n",
    "        all_metrics = {}\n",
    "        for metric_dict in self.test_results['metrics']:\n",
    "            for key, value in metric_dict.items():\n",
    "                if key not in all_metrics:\n",
    "                    all_metrics[key] = []\n",
    "                all_metrics[key].append(value)\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        aggregate = {}\n",
    "        for metric_name, values in all_metrics.items():\n",
    "            if values:\n",
    "                aggregate[metric_name] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'std': np.std(values),\n",
    "                    'min': np.min(values),\n",
    "                    'max': np.max(values),\n",
    "                    'median': np.median(values),\n",
    "                    'q25': np.percentile(values, 25),\n",
    "                    'q75': np.percentile(values, 75),\n",
    "                    'values': values\n",
    "                }\n",
    "        \n",
    "        return aggregate\n",
    "    \n",
    "    def _generate_comprehensive_visualizations(self, output_dir, aggregate_metrics):       #Generate comprehensive test visualizations\n",
    "\n",
    "        \n",
    "        print(\"\\n Generating comprehensive visualizations...\")\n",
    "        \n",
    "        # 1. Metrics dashboard\n",
    "        self._create_metrics_dashboard(output_dir, aggregate_metrics)\n",
    "        \n",
    "        # 2. Quality distribution analysis\n",
    "        self._create_quality_distribution(output_dir, aggregate_metrics)\n",
    "        \n",
    "        # 3. Performance correlation analysis\n",
    "        self._create_correlation_analysis(output_dir, aggregate_metrics)\n",
    "        \n",
    "        # 4. Best and worst examples\n",
    "        self._create_examples_showcase(output_dir, aggregate_metrics)\n",
    "        \n",
    "        # 5. Detailed statistical analysis\n",
    "        self._create_statistical_analysis(output_dir, aggregate_metrics)\n",
    "    \n",
    "    def _create_metrics_dashboard(self, output_dir, aggregate_metrics):\n",
    "        \"\"\"Create comprehensive metrics dashboard\"\"\"\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Summary statistics table\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        self._plot_summary_table(ax1, aggregate_metrics)\n",
    "        \n",
    "        # PSNR distribution\n",
    "        if 'psnr' in aggregate_metrics:\n",
    "            ax2 = fig.add_subplot(gs[0, 2:])\n",
    "            psnr_values = aggregate_metrics['psnr']['values']\n",
    "            ax2.hist(psnr_values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            ax2.axvline(aggregate_metrics['psnr']['mean'], color='red', linestyle='--', \n",
    "                       label=f\"Mean: {aggregate_metrics['psnr']['mean']:.2f}\")\n",
    "            ax2.axvline(aggregate_metrics['psnr']['median'], color='green', linestyle='--', \n",
    "                       label=f\"Median: {aggregate_metrics['psnr']['median']:.2f}\")\n",
    "            ax2.set_title('PSNR Distribution')\n",
    "            ax2.set_xlabel('PSNR (dB)')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # SSIM vs PSNR scatter\n",
    "        if 'psnr' in aggregate_metrics and 'ssim' in aggregate_metrics:\n",
    "            ax3 = fig.add_subplot(gs[1, :2])\n",
    "            psnr_vals = aggregate_metrics['psnr']['values']\n",
    "            ssim_vals = aggregate_metrics['ssim']['values']\n",
    "            scatter = ax3.scatter(psnr_vals, ssim_vals, alpha=0.6, c=psnr_vals, cmap='viridis')\n",
    "            ax3.set_xlabel('PSNR (dB)')\n",
    "            ax3.set_ylabel('SSIM')\n",
    "            ax3.set_title('PSNR vs SSIM Correlation')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            plt.colorbar(scatter, ax=ax3, label='PSNR (dB)')\n",
    "        \n",
    "        # Quality pie chart\n",
    "        if 'psnr' in aggregate_metrics:\n",
    "            ax4 = fig.add_subplot(gs[1, 2:])\n",
    "            self._plot_quality_pie_chart(ax4, aggregate_metrics['psnr']['values'])\n",
    "        \n",
    "        # Box plots for all metrics\n",
    "        ax5 = fig.add_subplot(gs[2, :])\n",
    "        self._plot_metrics_boxplots(ax5, aggregate_metrics)\n",
    "        \n",
    "        plt.suptitle('Test Evaluation Dashboard', fontsize=20, y=0.98)\n",
    "        plt.savefig(os.path.join(output_dir, 'test_evaluation_dashboard.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_summary_table(self, ax, aggregate_metrics):\n",
    "        \"\"\"Plot summary statistics table\"\"\"\n",
    "        table_data = []\n",
    "        \n",
    "        for metric_name, metric_data in aggregate_metrics.items():\n",
    "            if isinstance(metric_data, dict) and 'mean' in metric_data:\n",
    "                table_data.append([\n",
    "                    metric_name.upper(),\n",
    "                    f\"{metric_data['mean']:.4f}\",\n",
    "                    f\"{metric_data['std']:.4f}\",\n",
    "                    f\"{metric_data['median']:.4f}\",\n",
    "                    f\"{metric_data['min']:.4f}\",\n",
    "                    f\"{metric_data['max']:.4f}\"\n",
    "                ])\n",
    "        \n",
    "        if table_data:\n",
    "            headers = ['Metric', 'Mean', 'Std', 'Median', 'Min', 'Max']\n",
    "            table = ax.table(cellText=table_data, colLabels=headers, \n",
    "                           cellLoc='center', loc='center')\n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(9)\n",
    "            table.scale(1.2, 1.5)\n",
    "            \n",
    "            # Style the table\n",
    "            for i in range(len(headers)):\n",
    "                table[(0, i)].set_facecolor('#4CAF50')\n",
    "                table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        ax.set_title('Summary Statistics', fontsize=14, weight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    def _plot_quality_pie_chart(self, ax, psnr_values):\n",
    "        \"\"\"Plot quality distribution pie chart\"\"\"\n",
    "        excellent = sum(1 for x in psnr_values if x >= 35)\n",
    "        very_good = sum(1 for x in psnr_values if 30 <= x < 35)\n",
    "        good = sum(1 for x in psnr_values if 25 <= x < 30)\n",
    "        fair = sum(1 for x in psnr_values if 20 <= x < 25)\n",
    "        poor = sum(1 for x in psnr_values if x < 20)\n",
    "        \n",
    "        sizes = [excellent, very_good, good, fair, poor]\n",
    "        labels = ['Excellent\\n(≥35dB)', 'Very Good\\n(30-35dB)', 'Good\\n(25-30dB)', \n",
    "                 'Fair\\n(20-25dB)', 'Poor\\n(<20dB)']\n",
    "        colors = ['darkgreen', 'green', 'orange', 'gold', 'red']\n",
    "        \n",
    "        # Only show non-zero categories\n",
    "        non_zero_sizes = [(size, label, color) for size, label, color in zip(sizes, labels, colors) if size > 0]\n",
    "        if non_zero_sizes:\n",
    "            sizes, labels, colors = zip(*non_zero_sizes)\n",
    "            \n",
    "            wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, \n",
    "                                            autopct='%1.1f%%', startangle=90)\n",
    "            \n",
    "            # Enhance text\n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('white')\n",
    "                autotext.set_weight('bold')\n",
    "        \n",
    "        ax.set_title('Quality Distribution by PSNR', fontsize=12, weight='bold')\n",
    "    \n",
    "    def _plot_metrics_boxplots(self, ax, aggregate_metrics):\n",
    "        \"\"\"Plot box plots for all metrics\"\"\"\n",
    "        metrics_to_plot = ['psnr', 'ssim', 'mae', 'mse', 'lpips', 'edge_similarity']\n",
    "        available_metrics = [m for m in metrics_to_plot if m in aggregate_metrics]\n",
    "        \n",
    "        if available_metrics:\n",
    "            data_to_plot = []\n",
    "            labels = []\n",
    "            \n",
    "            for metric in available_metrics:\n",
    "                data_to_plot.append(aggregate_metrics[metric]['values'])\n",
    "                labels.append(metric.upper())\n",
    "            \n",
    "            bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "            \n",
    "            # Color the boxes\n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, len(bp['boxes'])))\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_title('Metrics Distribution (Box Plots)', fontsize=12, weight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _create_quality_distribution(self, output_dir, aggregate_metrics):\n",
    "        \"\"\"Create detailed quality distribution analysis\"\"\"\n",
    "        \n",
    "        if 'psnr' not in aggregate_metrics:\n",
    "            return\n",
    "        \n",
    "        psnr_values = aggregate_metrics['psnr']['values']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # PSNR histogram with quality zones\n",
    "        axes[0, 0].hist(psnr_values, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        \n",
    "        # Add quality zone backgrounds\n",
    "        axes[0, 0].axvspan(35, max(psnr_values), alpha=0.2, color='green', label='Excellent')\n",
    "        axes[0, 0].axvspan(30, 35, alpha=0.2, color='lightgreen', label='Very Good')\n",
    "        axes[0, 0].axvspan(25, 30, alpha=0.2, color='orange', label='Good')\n",
    "        axes[0, 0].axvspan(20, 25, alpha=0.2, color='gold', label='Fair')\n",
    "        axes[0, 0].axvspan(min(psnr_values), 20, alpha=0.2, color='red', label='Poor')\n",
    "        \n",
    "        axes[0, 0].set_title('PSNR Distribution with Quality Zones')\n",
    "        axes[0, 0].set_xlabel('PSNR (dB)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cumulative distribution\n",
    "        sorted_psnr = np.sort(psnr_values)\n",
    "        y_vals = np.arange(1, len(sorted_psnr) + 1) / len(sorted_psnr)\n",
    "        axes[0, 1].plot(sorted_psnr, y_vals, linewidth=2)\n",
    "        axes[0, 1].set_title('Cumulative PSNR Distribution')\n",
    "        axes[0, 1].set_xlabel('PSNR (dB)')\n",
    "        axes[0, 1].set_ylabel('Cumulative Probability')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Quality summary bar chart\n",
    "        quality_counts = {\n",
    "            'Excellent': sum(1 for x in psnr_values if x >= 35),\n",
    "            'Very Good': sum(1 for x in psnr_values if 30 <= x < 35),\n",
    "            'Good': sum(1 for x in psnr_values if 25 <= x < 30),\n",
    "            'Fair': sum(1 for x in psnr_values if 20 <= x < 25),\n",
    "            'Poor': sum(1 for x in psnr_values if x < 20)\n",
    "        }\n",
    "        \n",
    "        qualities = list(quality_counts.keys())\n",
    "        counts = list(quality_counts.values())\n",
    "        colors = ['darkgreen', 'green', 'orange', 'gold', 'red']\n",
    "        \n",
    "        bars = axes[1, 0].bar(qualities, counts, color=colors, alpha=0.7)\n",
    "        axes[1, 0].set_title('Quality Distribution Count')\n",
    "        axes[1, 0].set_ylabel('Number of Images')\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            if count > 0:\n",
    "                axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                               str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Performance percentiles\n",
    "        percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "        psnr_percentiles = [np.percentile(psnr_values, p) for p in percentiles]\n",
    "        \n",
    "        axes[1, 1].plot(percentiles, psnr_percentiles, 'o-', linewidth=2, markersize=8)\n",
    "        axes[1, 1].set_title('PSNR Percentiles')\n",
    "        axes[1, 1].set_xlabel('Percentile')\n",
    "        axes[1, 1].set_ylabel('PSNR (dB)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add percentile labels\n",
    "        for p, psnr in zip(percentiles, psnr_percentiles):\n",
    "            axes[1, 1].annotate(f'{psnr:.1f}', (p, psnr), textcoords=\"offset points\", \n",
    "                               xytext=(0,10), ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'quality_assessment', 'quality_distribution_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_correlation_analysis(self, output_dir, aggregate_metrics):\n",
    "        \"\"\"Create correlation analysis between metrics\"\"\"\n",
    "        \n",
    "        # Prepare data for correlation analysis\n",
    "        metrics_data = {}\n",
    "        for metric_name, metric_data in aggregate_metrics.items():\n",
    "            if isinstance(metric_data, dict) and 'values' in metric_data:\n",
    "                metrics_data[metric_name] = metric_data['values']\n",
    "        \n",
    "        if len(metrics_data) < 2:\n",
    "            return\n",
    "        \n",
    "        # Create correlation matrix\n",
    "        df = pd.DataFrame(metrics_data)\n",
    "        correlation_matrix = df.corr()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Correlation heatmap\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5, ax=axes[0])\n",
    "        axes[0].set_title('Metrics Correlation Matrix', fontsize=14, weight='bold')\n",
    "        \n",
    "        # Pairwise scatter plots for key metrics\n",
    "        if 'psnr' in metrics_data and 'ssim' in metrics_data:\n",
    "            axes[1].scatter(metrics_data['psnr'], metrics_data['ssim'], alpha=0.6, s=50)\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(metrics_data['psnr'], metrics_data['ssim'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[1].plot(metrics_data['psnr'], p(metrics_data['psnr']), \"r--\", alpha=0.8)\n",
    "            \n",
    "            # Calculate correlation coefficient\n",
    "            corr_coef = np.corrcoef(metrics_data['psnr'], metrics_data['ssim'])[0, 1]\n",
    "            axes[1].text(0.05, 0.95, f'Correlation: {corr_coef:.3f}', \n",
    "                        transform=axes[1].transAxes, fontsize=12, \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "            \n",
    "            axes[1].set_xlabel('PSNR (dB)')\n",
    "            axes[1].set_ylabel('SSIM')\n",
    "            axes[1].set_title('PSNR vs SSIM Correlation', fontsize=14, weight='bold')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'metrics_analysis', 'correlation_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_examples_showcase(self, output_dir, aggregate_metrics):\n",
    "        \"\"\"Create showcase of best and worst examples\"\"\"\n",
    "        \n",
    "        if 'psnr' not in aggregate_metrics:\n",
    "            return\n",
    "        \n",
    "        psnr_values = aggregate_metrics['psnr']['values']\n",
    "        \n",
    "        # Find best and worst examples\n",
    "        best_indices = np.argsort(psnr_values)[-5:][::-1]  # Top 5\n",
    "        worst_indices = np.argsort(psnr_values)[:5]  # Bottom 5\n",
    "        \n",
    "        # Create showcase for best examples\n",
    "        self._create_examples_grid(best_indices, \"Best\", output_dir, \"📈 Top Performing Examples\")\n",
    "        \n",
    "        # Create showcase for worst examples\n",
    "        self._create_examples_grid(worst_indices, \"Worst\", output_dir, \"📉 Lowest Performing Examples\")\n",
    "    \n",
    "    def _create_examples_grid(self, indices, category, output_dir, title):\n",
    "        \"\"\"Create a grid showcasing examples\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(len(indices), 4, figsize=(16, 4*len(indices)))\n",
    "        if len(indices) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            filename = self.test_results['filenames'][idx]\n",
    "            metrics = self.test_results['metrics'][idx]\n",
    "            \n",
    "            # Load the comparison image if it exists\n",
    "            comparison_path = os.path.join(output_dir, 'comparisons', f'{filename}_detailed_comparison.png')\n",
    "            if os.path.exists(comparison_path):\n",
    "                comparison_img = plt.imread(comparison_path)\n",
    "                \n",
    "                # Split the comparison image (assuming 3 panels + metrics area)\n",
    "                h, w = comparison_img.shape[:2]\n",
    "                panel_width = w // 3\n",
    "                \n",
    "                input_img = comparison_img[80:, :panel_width]  # Skip text area\n",
    "                generated_img = comparison_img[80:, panel_width:2*panel_width]\n",
    "                target_img = comparison_img[80:, 2*panel_width:3*panel_width]\n",
    "                \n",
    "                # Plot images\n",
    "                axes[i, 0].imshow(input_img)\n",
    "                axes[i, 0].set_title('Input', fontsize=12)\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                axes[i, 1].imshow(generated_img)\n",
    "                axes[i, 1].set_title('Generated', fontsize=12)\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                axes[i, 2].imshow(target_img)\n",
    "                axes[i, 2].set_title('Target', fontsize=12)\n",
    "                axes[i, 2].axis('off')\n",
    "                \n",
    "                # Metrics text\n",
    "                metrics_text = f\"Filename: {filename}\\n\\n\"\n",
    "                metrics_text += f\"PSNR: {metrics['psnr']:.2f} dB\\n\"\n",
    "                metrics_text += f\"SSIM: {metrics['ssim']:.3f}\\n\"\n",
    "                metrics_text += f\"MAE: {metrics['mae']:.4f}\\n\"\n",
    "                metrics_text += f\"MSE: {metrics['mse']:.4f}\\n\"\n",
    "                metrics_text += f\"LPIPS: {metrics['lpips']:.4f}\\n\"\n",
    "                metrics_text += f\"Edge Sim: {metrics['edge_similarity']:.3f}\"\n",
    "                \n",
    "                axes[i, 3].text(0.1, 0.9, metrics_text, transform=axes[i, 3].transAxes, \n",
    "                               fontsize=10, verticalalignment='top',\n",
    "                               bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "                axes[i, 3].set_title('Metrics', fontsize=12)\n",
    "                axes[i, 3].axis('off')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16, weight='bold', y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'quality_assessment', f'{category.lower()}_examples_showcase.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_statistical_analysis(self, output_dir, aggregate_metrics):\n",
    "        \"\"\"Create detailed statistical analysis\"\"\"\n",
    "        \n",
    "        # Create statistical summary report\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Metrics comparison violin plot\n",
    "        if len(aggregate_metrics) > 1:\n",
    "            ax1 = axes[0, 0]\n",
    "            metrics_to_plot = ['psnr', 'ssim', 'mae']\n",
    "            available_metrics = [m for m in metrics_to_plot if m in aggregate_metrics]\n",
    "            \n",
    "            if available_metrics:\n",
    "                data_normalized = []\n",
    "                labels = []\n",
    "                \n",
    "                for metric in available_metrics:\n",
    "                    values = aggregate_metrics[metric]['values']\n",
    "                    # Normalize to 0-1 for comparison\n",
    "                    if metric == 'psnr':\n",
    "                        normalized = np.array(values) / 40.0  # Assume 40dB is excellent\n",
    "                    elif metric == 'ssim':\n",
    "                        normalized = np.array(values)  # Already 0-1\n",
    "                    elif metric == 'mae':\n",
    "                        normalized = 1 - np.array(values)  # Invert (lower is better)\n",
    "                    else:\n",
    "                        normalized = np.array(values)\n",
    "                    \n",
    "                    data_normalized.append(normalized)\n",
    "                    labels.append(metric.upper())\n",
    "                \n",
    "                parts = ax1.violinplot(data_normalized, positions=range(len(labels)), showmeans=True)\n",
    "                ax1.set_xticks(range(len(labels)))\n",
    "                ax1.set_xticklabels(labels)\n",
    "                ax1.set_title('Normalized Metrics Distribution', fontsize=12, weight='bold')\n",
    "                ax1.set_ylabel('Normalized Score (0-1)')\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Performance ranking\n",
    "        ax2 = axes[0, 1]\n",
    "        if 'psnr' in aggregate_metrics:\n",
    "            psnr_values = aggregate_metrics['psnr']['values']\n",
    "            filenames = self.test_results['filenames']\n",
    "            \n",
    "            # Get top 10 and bottom 10\n",
    "            sorted_indices = np.argsort(psnr_values)\n",
    "            top_10_idx = sorted_indices[-10:][::-1]\n",
    "            bottom_10_idx = sorted_indices[:10]\n",
    "            \n",
    "            # Plot ranking\n",
    "            y_pos_top = np.arange(len(top_10_idx))\n",
    "            y_pos_bottom = np.arange(len(bottom_10_idx)) - len(bottom_10_idx) - 1\n",
    "            \n",
    "            ax2.barh(y_pos_top, [psnr_values[i] for i in top_10_idx], \n",
    "                    color='green', alpha=0.7, label='Top 10')\n",
    "            ax2.barh(y_pos_bottom, [psnr_values[i] for i in bottom_10_idx], \n",
    "                    color='red', alpha=0.7, label='Bottom 10')\n",
    "            \n",
    "            # Add labels\n",
    "            top_labels = [filenames[i][:15] + '...' if len(filenames[i]) > 15 else filenames[i] \n",
    "                         for i in top_10_idx]\n",
    "            bottom_labels = [filenames[i][:15] + '...' if len(filenames[i]) > 15 else filenames[i] \n",
    "                           for i in bottom_10_idx]\n",
    "            \n",
    "            ax2.set_yticks(list(y_pos_top) + list(y_pos_bottom))\n",
    "            ax2.set_yticklabels(top_labels + bottom_labels, fontsize=8)\n",
    "            ax2.set_xlabel('PSNR (dB)')\n",
    "            ax2.set_title('Performance Ranking (Top/Bottom 10)', fontsize=12, weight='bold')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Outlier analysis\n",
    "        ax3 = axes[1, 0]\n",
    "        if 'psnr' in aggregate_metrics:\n",
    "            psnr_values = aggregate_metrics['psnr']['values']\n",
    "            \n",
    "            # Calculate IQR and outliers\n",
    "            q1 = np.percentile(psnr_values, 25)\n",
    "            q3 = np.percentile(psnr_values, 75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            outliers_low = [x for x in psnr_values if x < lower_bound]\n",
    "            outliers_high = [x for x in psnr_values if x > upper_bound]\n",
    "            \n",
    "            # Plot box plot with outliers highlighted\n",
    "            bp = ax3.boxplot(psnr_values, patch_artist=True)\n",
    "            bp['boxes'][0].set_facecolor('lightblue')\n",
    "            \n",
    "            # Highlight outliers\n",
    "            if outliers_low:\n",
    "                ax3.scatter([1] * len(outliers_low), outliers_low, color='red', s=50, alpha=0.7, label='Low Outliers')\n",
    "            if outliers_high:\n",
    "                ax3.scatter([1] * len(outliers_high), outliers_high, color='green', s=50, alpha=0.7, label='High Outliers')\n",
    "            \n",
    "            ax3.set_title('Outlier Analysis (PSNR)', fontsize=12, weight='bold')\n",
    "            ax3.set_ylabel('PSNR (dB)')\n",
    "            if outliers_low or outliers_high:\n",
    "                ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Summary statistics table\n",
    "        ax4 = axes[1, 1]\n",
    "        \n",
    "        # Prepare comprehensive statistics\n",
    "        stats_data = []\n",
    "        for metric_name, metric_data in aggregate_metrics.items():\n",
    "            if isinstance(metric_data, dict) and 'mean' in metric_data:\n",
    "                stats_data.append([\n",
    "                    metric_name.upper(),\n",
    "                    f\"{metric_data['mean']:.4f}\",\n",
    "                    f\"{metric_data['std']:.4f}\",\n",
    "                    f\"{metric_data['q25']:.4f}\",\n",
    "                    f\"{metric_data['median']:.4f}\",\n",
    "                    f\"{metric_data['q75']:.4f}\"\n",
    "                ])\n",
    "        \n",
    "        if stats_data:\n",
    "            headers = ['Metric', 'Mean', 'Std', 'Q25', 'Median', 'Q75']\n",
    "            table = ax4.table(cellText=stats_data, colLabels=headers, \n",
    "                             cellLoc='center', loc='center')\n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(10)\n",
    "            table.scale(1.2, 1.8)\n",
    "            \n",
    "            # Style the table\n",
    "            for i in range(len(headers)):\n",
    "                table[(0, i)].set_facecolor('#2E8B57')\n",
    "                table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        ax4.set_title('Detailed Statistics', fontsize=12, weight='bold')\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'metrics_analysis', 'statistical_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _save_comprehensive_results(self, output_dir, aggregate_metrics):\n",
    "        \"\"\"Save comprehensive test results\"\"\"\n",
    "        \n",
    "        # Save detailed JSON results\n",
    "        results_summary = {\n",
    "            'evaluation_summary': {\n",
    "                'total_images': len(self.test_results['metrics']),\n",
    "                'model_path': self.model_path,\n",
    "                'evaluation_date': datetime.now().isoformat(),\n",
    "                'configuration': {\n",
    "                    'image_size': self.config.image_size,\n",
    "                    'device': str(self.config.device),\n",
    "                    'data_root': self.config.data_root\n",
    "                }\n",
    "            },\n",
    "            'aggregate_metrics': aggregate_metrics,\n",
    "            'individual_results': self.test_results['individual_results']\n",
    "        }\n",
    "        \n",
    "        # Save to JSON\n",
    "        results_path = os.path.join(output_dir, 'detailed_reports', 'comprehensive_test_results.json')\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results_summary, f, indent=2, default=str)\n",
    "        \n",
    "        # Save to CSV for analysis\n",
    "        if self.test_results['individual_results']:\n",
    "            df_data = []\n",
    "            for result in self.test_results['individual_results']:\n",
    "                row = {\n",
    "                    'filename': result['filename'],\n",
    "                    'index': result['index']\n",
    "                }\n",
    "                row.update(result['metrics'])\n",
    "                df_data.append(row)\n",
    "            \n",
    "            df = pd.DataFrame(df_data)\n",
    "            csv_path = os.path.join(output_dir, 'detailed_reports', 'test_metrics_detailed.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Create comprehensive markdown report\n",
    "        self._create_comprehensive_report(output_dir, aggregate_metrics, results_summary)\n",
    "        \n",
    "        logger.info(f\"📊 Comprehensive test results saved to {output_dir}\")\n",
    "    \n",
    "    def _create_comprehensive_report(self, output_dir, aggregate_metrics, results_summary):\n",
    "        \"\"\"Create comprehensive markdown report\"\"\"\n",
    "        \n",
    "        report_path = os.path.join(output_dir, 'COMPREHENSIVE_TEST_REPORT.md')\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"# 🧪 Comprehensive Pix2Pix Test Evaluation Report\\n\\n\")\n",
    "            f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            f.write(f\"**Model:** `{self.model_path}`\\n\\n\")\n",
    "            f.write(f\"**Dataset:** {self.config.data_root}\\n\\n\")\n",
    "            f.write(f\"**Total Test Images:** {results_summary['evaluation_summary']['total_images']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"---\\n\\n\")\n",
    "            f.write(\"##  Executive Summary\\n\\n\")\n",
    "            \n",
    "            # Key findings\n",
    "            if 'psnr' in aggregate_metrics:\n",
    "                psnr_data = aggregate_metrics['psnr']\n",
    "                f.write(f\"- **Average PSNR:** {psnr_data['mean']:.2f} dB (± {psnr_data['std']:.2f})\\n\")\n",
    "                f.write(f\"- **PSNR Range:** {psnr_data['min']:.2f} - {psnr_data['max']:.2f} dB\\n\")\n",
    "            \n",
    "            if 'ssim' in aggregate_metrics:\n",
    "                ssim_data = aggregate_metrics['ssim']\n",
    "                f.write(f\"- **Average SSIM:** {ssim_data['mean']:.3f} (± {ssim_data['std']:.3f})\\n\")\n",
    "            \n",
    "            # Quality assessment\n",
    "            if 'psnr' in aggregate_metrics:\n",
    "                psnr_values = aggregate_metrics['psnr']['values']\n",
    "                excellent = sum(1 for x in psnr_values if x >= 35)\n",
    "                very_good = sum(1 for x in psnr_values if 30 <= x < 35)\n",
    "                good = sum(1 for x in psnr_values if 25 <= x < 30)\n",
    "                fair = sum(1 for x in psnr_values if 20 <= x < 25)\n",
    "                poor = sum(1 for x in psnr_values if x < 20)\n",
    "                total = len(psnr_values)\n",
    "                \n",
    "                f.write(f\"\\n### 🎯 Quality Distribution\\n\\n\")\n",
    "                f.write(f\"| Quality Level | Count | Percentage |\\n\")\n",
    "                f.write(f\"|---------------|-------|------------|\\n\")\n",
    "                f.write(f\"| Excellent (≥35dB) | {excellent} | {excellent/total*100:.1f}% |\\n\")\n",
    "                f.write(f\"| Very Good (30-35dB) | {very_good} | {very_good/total*100:.1f}% |\\n\")\n",
    "                f.write(f\"| Good (25-30dB) | {good} | {good/total*100:.1f}% |\\n\")\n",
    "                f.write(f\"| Fair (20-25dB) | {fair} | {fair/total*100:.1f}% |\\n\")\n",
    "                f.write(f\"| Poor (<20dB) | {poor} | {poor/total*100:.1f}% |\\n\")\n",
    "            \n",
    "            f.write(f\"\\n---\\n\\n\")\n",
    "            f.write(\"## 📈 Detailed Metrics Analysis\\n\\n\")\n",
    "            \n",
    "            # Comprehensive metrics table\n",
    "            f.write(\"### Summary Statistics\\n\\n\")\n",
    "            f.write(\"| Metric | Mean | Std | Min | Max | Median | Q25 | Q75 |\\n\")\n",
    "            f.write(\"|--------|------|-----|-----|-----|--------|-----|-----|\\n\")\n",
    "            \n",
    "            for metric_name, metric_data in aggregate_metrics.items():\n",
    "                if isinstance(metric_data, dict) and 'mean' in metric_data:\n",
    "                    f.write(f\"| {metric_name.upper()} | {metric_data['mean']:.4f} | \"\n",
    "                           f\"{metric_data['std']:.4f} | {metric_data['min']:.4f} | \"\n",
    "                           f\"{metric_data['max']:.4f} | {metric_data['median']:.4f} | \"\n",
    "                           f\"{metric_data['q25']:.4f} | {metric_data['q75']:.4f} |\\n\")\n",
    "            \n",
    "            f.write(f\"\\n### 🔍 Key Insights\\n\\n\")\n",
    "            \n",
    "            # Generate insights based on metrics\n",
    "            if 'psnr' in aggregate_metrics and 'ssim' in aggregate_metrics:\n",
    "                psnr_mean = aggregate_metrics['psnr']['mean']\n",
    "                ssim_mean = aggregate_metrics['ssim']['mean']\n",
    "                \n",
    "                if psnr_mean >= 30:\n",
    "                    f.write(\"✅ **Excellent overall performance** - High PSNR indicates very good reconstruction quality.\\n\\n\")\n",
    "                elif psnr_mean >= 25:\n",
    "                    f.write(\"✅ **Good performance** - PSNR values show satisfactory reconstruction quality.\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\"⚠️ **Room for improvement** - PSNR suggests reconstruction quality could be enhanced.\\n\\n\")\n",
    "                \n",
    "                if ssim_mean >= 0.8:\n",
    "                    f.write(\"✅ **Strong structural similarity** - Generated images preserve structural information well.\\n\\n\")\n",
    "                elif ssim_mean >= 0.6:\n",
    "                    f.write(\"✅ **Moderate structural preservation** - Generated images maintain reasonable structural fidelity.\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\"⚠️ **Structural improvements needed** - Consider enhancing structural preservation in the model.\\n\\n\")\n",
    "            \n",
    "            f.write(\"---\\n\\n\")\n",
    "            f.write(\"## 🏆 Performance Examples\\n\\n\")\n",
    "            \n",
    "            # Best examples\n",
    "            if 'psnr' in aggregate_metrics:\n",
    "                psnr_values = aggregate_metrics['psnr']['values']\n",
    "                best_indices = np.argsort(psnr_values)[-5:][::-1]\n",
    "                worst_indices = np.argsort(psnr_values)[:5]\n",
    "                \n",
    "                f.write(\"### 🥇 Top Performing Images\\n\\n\")\n",
    "                for i, idx in enumerate(best_indices, 1):\n",
    "                    filename = self.test_results['filenames'][idx]\n",
    "                    psnr_val = psnr_values[idx]\n",
    "                    ssim_val = aggregate_metrics['ssim']['values'][idx] if 'ssim' in aggregate_metrics else \"N/A\"\n",
    "                    f.write(f\"{i}. **{filename}** - PSNR: {psnr_val:.2f}dB, SSIM: {ssim_val:.3f if ssim_val != 'N/A' else 'N/A'}\\n\")\n",
    "                \n",
    "                f.write(\"\\n### 📉 Areas for Improvement\\n\\n\")\n",
    "                for i, idx in enumerate(worst_indices, 1):\n",
    "                    filename = self.test_results['filenames'][idx]\n",
    "                    psnr_val = psnr_values[idx]\n",
    "                    ssim_val = aggregate_metrics['ssim']['values'][idx] if 'ssim' in aggregate_metrics else \"N/A\"\n",
    "                    f.write(f\"{i}. **{filename}** - PSNR: {psnr_val:.2f}dB, SSIM: {ssim_val:.3f if ssim_val != 'N/A' else 'N/A'}\\n\")\n",
    "            \n",
    "            f.write(f\"\\n---\\n\\n\")\n",
    "            f.write(\"## 📁 Generated Files\\n\\n\")\n",
    "            f.write(\"This evaluation generated the following analysis files:\\n\\n\")\n",
    "            f.write(\"### 🖼️ Visual Results\\n\")\n",
    "            f.write(\"- `generated_images/` - Individual generated images\\n\")\n",
    "            f.write(\"- `comparisons/` - Side-by-side input/generated/target comparisons\\n\")\n",
    "            f.write(\"- `error_maps/` - Pixel-wise error analysis visualizations\\n\\n\")\n",
    "            \n",
    "            f.write(\"### 📊 Analysis Reports\\n\")\n",
    "            f.write(\"- `test_evaluation_dashboard.png` - Comprehensive metrics dashboard\\n\")\n",
    "            f.write(\"- `quality_assessment/quality_distribution_analysis.png` - Quality distribution analysis\\n\")\n",
    "            f.write(\"- `metrics_analysis/correlation_analysis.png` - Metrics correlation analysis\\n\")\n",
    "            f.write(\"- `metrics_analysis/statistical_analysis.png` - Detailed statistical analysis\\n\")\n",
    "            f.write(\"- `quality_assessment/best_examples_showcase.png` - Top performing examples\\n\")\n",
    "            f.write(\"- `quality_assessment/worst_examples_showcase.png` - Examples needing improvement\\n\\n\")\n",
    "            \n",
    "            f.write(\"### 📋 Data Files\\n\")\n",
    "            f.write(\"- `detailed_reports/comprehensive_test_results.json` - Complete results in JSON format\\n\")\n",
    "            f.write(\"- `detailed_reports/test_metrics_detailed.csv` - Metrics data for further analysis\\n\")\n",
    "            f.write(\"- `COMPREHENSIVE_TEST_REPORT.md` - This detailed report\\n\\n\")\n",
    "            \n",
    "            f.write(\"---\\n\\n\")\n",
    "            f.write(\"## 🎯 Recommendations\\n\\n\")\n",
    "            \n",
    "            # Generate recommendations based on results\n",
    "            if 'psnr' in aggregate_metrics:\n",
    "                psnr_std = aggregate_metrics['psnr']['std']\n",
    "                psnr_mean = aggregate_metrics['psnr']['mean']\n",
    "                \n",
    "                if psnr_std > 5:\n",
    "                    f.write(\"1. **Consistency Improvement**: High PSNR variance suggests inconsistent performance across images. Consider data augmentation or model regularization.\\n\\n\")\n",
    "                \n",
    "                if psnr_mean < 25:\n",
    "                    f.write(\"2. **Quality Enhancement**: Consider increasing model capacity, adjusting loss functions, or extending training duration.\\n\\n\")\n",
    "                \n",
    "                if 'mae' in aggregate_metrics and aggregate_metrics['mae']['mean'] > 0.1:\n",
    "                    f.write(\"3. **Reconstruction Accuracy**: High MAE suggests room for improvement in pixel-level accuracy. Consider adjusting the L1 loss weight.\\n\\n\")\n",
    "            \n",
    "            f.write(\"4. **Focus Areas**: Pay special attention to images in the 'Areas for Improvement' section for targeted enhancements.\\n\\n\")\n",
    "            f.write(\"5. **Validation**: Use the top-performing examples as benchmarks for future model iterations.\\n\\n\")\n",
    "            \n",
    "            f.write(\"---\\n\\n\")\n",
    "            f.write(\"*Report generated by Enhanced Pix2Pix Evaluation System*\\n\")\n",
    "\n",
    "def train_pix2pix_complete_pipeline():  #MAIN TRAINING FUNCTION WITH COMPREHENSIVE PIPELINE\n",
    "    \"\"\"Complete training pipeline with train/validation/test splits\"\"\"\n",
    "    \n",
    "    print(\"🚀 \" + \"=\"*70)\n",
    "    print(\"🚀 ENHANCED PIX2PIX - COMPLETE PIPELINE WITH TRAIN/VAL/TEST\")\n",
    "    print(\"🚀 \" + \"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        config = Config()\n",
    "        \n",
    "        # Data splitting and preparation\n",
    "        logger.info(\"📊 Preparing data splits...\")\n",
    "        data_splitter = DataSplitter(config)\n",
    "        train_pairs, val_pairs, test_pairs = data_splitter.find_and_split_data()\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Pix2PixDataset(train_pairs, config, 'train')\n",
    "        val_dataset = Pix2PixDataset(val_pairs, config, 'val')\n",
    "        test_dataset = Pix2PixDataset(test_pairs, config, 'test')\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=config.num_workers, drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=config.num_workers, drop_last=False\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=config.num_workers, drop_last=False\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"📊 Dataloaders created:\")\n",
    "        logger.info(f\"   📚 Training: {len(train_loader)} batches\")\n",
    "        logger.info(f\"   🔍 Validation: {len(val_loader)} batches\")\n",
    "        logger.info(f\"   🧪 Testing: {len(test_loader)} batches\")\n",
    "        \n",
    "        # Initialize trainer\n",
    "        logger.info(\" Initializing enhanced trainer...\")\n",
    "        trainer = EnhancedTrainer(config)\n",
    "        \n",
    "        # Training loop with validation\n",
    "        logger.info(\"🏋️ Starting training with validation...\")\n",
    "        \n",
    "        for epoch in range(1, config.num_epochs + 1):\n",
    "            try:\n",
    "                print(f\"\\n📅 Epoch {epoch}/{config.num_epochs}\")\n",
    "                \n",
    "                # Training phase\n",
    "                train_results = trainer.train_epoch(train_loader, epoch)\n",
    "                \n",
    "                # Validation phase\n",
    "                val_results = trainer.validate_epoch(val_loader, epoch)\n",
    "                \n",
    "                # Update history\n",
    "                trainer.update_history(train_results, val_results)\n",
    "                \n",
    "                # Log results\n",
    "                logger.info(f\"📊 Epoch {epoch} Results:\")\n",
    "                logger.info(f\"   🏋️ Train - G_Loss: {train_results['g_loss']:.4f}, D_Loss: {train_results['d_loss']:.4f}, PSNR: {train_results['psnr']:.2f}dB\")\n",
    "                if val_results:\n",
    "                    logger.info(f\"   🔍 Val   - G_Loss: {val_results['g_loss']:.4f}, D_Loss: {val_results['d_loss']:.4f}, PSNR: {val_results['psnr']:.2f}dB, SSIM: {val_results['ssim']:.3f}\")\n",
    "                \n",
    "                # Check for best model\n",
    "                is_best = False\n",
    "                if val_results and val_results['psnr'] > trainer.best_val_psnr:\n",
    "                    trainer.best_val_psnr = val_results['psnr']\n",
    "                    trainer.best_epoch = epoch\n",
    "                    is_best = True\n",
    "                \n",
    "                # Save samples and checkpoints\n",
    "                if epoch % config.save_frequency == 0:\n",
    "                    trainer.generate_samples(train_loader, epoch, split_name='train')\n",
    "                    if val_loader:\n",
    "                        trainer.generate_samples(val_loader, epoch, split_name='val')\n",
    "                    trainer.plot_training_progress()\n",
    "                \n",
    "                if epoch % config.save_frequency == 0 or is_best:\n",
    "                    trainer.save_checkpoint(epoch, is_best)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\" Error in epoch {epoch}: {e}\")\n",
    "                if epoch > 10:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        # Training completed\n",
    "        print(\"\\n🎉 \" + \"=\"*70)\n",
    "        print(\"🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"🎉 \" + \"=\"*70)\n",
    "        \n",
    "        # Final training progress\n",
    "        trainer.plot_training_progress()\n",
    "        \n",
    "        # Comprehensive test evaluation\n",
    "        print(\"\\n🧪 \" + \"=\"*70)\n",
    "        print(\"🧪 STARTING COMPREHENSIVE TEST EVALUATION\")\n",
    "        print(\"🧪 \" + \"=\"*70)\n",
    "        \n",
    "        # Load best model for testing\n",
    "        best_model_path = os.path.join(config.experiment_dir, 'checkpoints', 'best_model.pth')\n",
    "        if os.path.exists(best_model_path):\n",
    "            test_evaluator = TestEvaluator(config, best_model_path)\n",
    "            test_results = test_evaluator.evaluate_test_dataset(test_loader)\n",
    "            \n",
    "            print(\"\\n📊 TEST EVALUATION SUMMARY:\")\n",
    "            if test_results:\n",
    "                for metric, data in test_results.items():\n",
    "                    if isinstance(data, dict) and 'mean' in data:\n",
    "                        print(f\"   {metric.upper()}: {data['mean']:.4f} ± {data['std']:.4f}\")\n",
    "        else:\n",
    "            logger.warning(\"⚠️ Best model not found, skipping test evaluation\")\n",
    "        \n",
    "        # Save final comprehensive summary\n",
    "        final_summary = {\n",
    "            'training_summary': {\n",
    "                'total_epochs': config.num_epochs,\n",
    "                'best_val_psnr': trainer.best_val_psnr,\n",
    "                'best_epoch': trainer.best_epoch,\n",
    "                'final_train_psnr': trainer.history['train_psnr'][-1] if trainer.history['train_psnr'] else 0,\n",
    "                'final_val_psnr': trainer.history['val_psnr'][-1] if trainer.history['val_psnr'] else 0\n",
    "            },\n",
    "            'data_split_summary': {\n",
    "                'total_images': len(train_pairs) + len(val_pairs) + len(test_pairs),\n",
    "                'train_images': len(train_pairs),\n",
    "                'val_images': len(val_pairs),\n",
    "                'test_images': len(test_pairs)\n",
    "            },\n",
    "            'test_results': test_results if 'test_results' in locals() else None,\n",
    "            'configuration': {\n",
    "                'data_root': config.data_root,\n",
    "                'image_size': config.image_size,\n",
    "                'batch_size': config.batch_size,\n",
    "                'learning_rate': config.learning_rate_g,\n",
    "                'train_ratio': config.train_ratio,\n",
    "                'val_ratio': config.val_ratio,\n",
    "                'test_ratio': config.test_ratio\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        summary_path = os.path.join(config.experiment_dir, 'final_results', 'complete_pipeline_summary.json')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(final_summary, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n FINAL PIPELINE RESULTS:\")\n",
    "        print(f\"    Best Validation PSNR: {trainer.best_val_psnr:.2f} dB (Epoch {trainer.best_epoch})\")\n",
    "        print(f\"    Training Images: {len(train_pairs)}\")\n",
    "        print(f\"    Validation Images: {len(val_pairs)}\")\n",
    "        print(f\"    Test Images: {len(test_pairs)}\")\n",
    "        print(f\"    Results saved to: {config.experiment_dir}\")\n",
    "        \n",
    "        print(f\"\\n COMPLETE PIPELINE OUTPUTS:\")\n",
    "        print(f\"    Training samples: {config.experiment_dir}/generated_samples/\")\n",
    "        print(f\"    Model checkpoints: {config.experiment_dir}/checkpoints/\")\n",
    "        print(f\"    Training progress: {config.experiment_dir}/training_progress/\")\n",
    "        print(f\"    Data splits info: {config.experiment_dir}/data_splits/\")\n",
    "        print(f\"    Validation results: {config.experiment_dir}/validation_results/\")\n",
    "        print(f\"    Test evaluation: {config.experiment_dir}/test_results/\")\n",
    "        print(f\"    Final summary: {config.experiment_dir}/final_results/\")\n",
    "        \n",
    "        return config.experiment_dir, final_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Pipeline failed: {e}\")\n",
    "        logger.error(f\"💡 Traceback: {traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "def main_complete(): #MAIN EXECUTION WITH ENHANCED FEATURES\n",
    "    \"\"\"Main execution function with complete pipeline\"\"\"\n",
    "    \n",
    "    print(\"🎮 \" + \"=\"*80)\n",
    "    print(\"🎮 ENHANCED PIX2PIX - COMPLETE TRAIN/VAL/TEST PIPELINE\")\n",
    "    print(\"🎮 \" + \"=\"*80)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "📋 ENHANCED FEATURES:\n",
    "\n",
    "🔄 **Automatic Data Splitting:**\n",
    "   - Intelligent train/validation/test splits ({TRAIN_RATIO*100:.0f}%/{VAL_RATIO*100:.0f}%/{TEST_RATIO*100:.0f}%)\n",
    "   - Reproducible splits with fixed random seed\n",
    "   - Comprehensive data validation\n",
    "\n",
    "📊 **Advanced Training:**\n",
    "   - Training with validation monitoring\n",
    "   - Early stopping based on validation PSNR\n",
    "   - Comprehensive loss tracking (Generator + Discriminator)\n",
    "   - Advanced data augmentation for training set\n",
    "\n",
    "📈 **Real-time Monitoring:**\n",
    "   - Training vs Validation curves\n",
    "   - Multiple metrics tracking (PSNR, SSIM, MAE, MSE)\n",
    "   - Best model checkpointing\n",
    "   - Progress visualization\n",
    "\n",
    "🧪 **Comprehensive Test Evaluation:**\n",
    "   - Detailed metrics analysis (6+ metrics)\n",
    "   - Quality distribution assessment\n",
    "   - Error map visualizations\n",
    "   - Statistical analysis and outlier detection\n",
    "   - Performance correlation analysis\n",
    "   - Best/worst examples showcase\n",
    "\n",
    "📁 **Professional Reporting:**\n",
    "   - Comprehensive markdown reports\n",
    "   - JSON/CSV data exports\n",
    "   - Interactive visualizations\n",
    "   - Executive summaries with insights\n",
    "\n",
    "🎯 **Data Organization:**\n",
    "   Data structure: {DATA_ROOT}/\n",
    "   ├── input/  (or existing folder names)\n",
    "   ├── target/ (or existing folder names)\n",
    "   \n",
    "   Results: {OUTPUT_FOLDER}/experiment_[timestamp]/\n",
    "   ├── checkpoints/\n",
    "   ├── data_splits/\n",
    "   ├── training_progress/\n",
    "   ├── validation_results/\n",
    "   ├── test_results/\n",
    "   └── final_results/\n",
    "\"\"\")\n",
    "    \n",
    "    try:\n",
    "        # Validate data paths\n",
    "        if not os.path.exists(DATA_ROOT):\n",
    "            print(f\"⚠️  DATA ROOT NOT FOUND: {DATA_ROOT}\")\n",
    "            print(\"\\n🔧 SETUP INSTRUCTIONS:\")\n",
    "            print(f\"1. Create data root: {DATA_ROOT}\")\n",
    "            print(f\"2. Add subfolders: input/ and target/ (or use existing folder structure)\")\n",
    "            print(\"3. Place matching image pairs in both folders\")\n",
    "            print(\"4. Run this script again\")\n",
    "            print(\"\\n💡 The script will auto-detect existing folder structures\")\n",
    "            return\n",
    "        \n",
    "        # Start complete pipeline\n",
    "        print(\"\\n🚀 Starting complete pipeline...\")\n",
    "        experiment_dir, summary = train_pix2pix_complete_pipeline()\n",
    "        \n",
    "        print(\"\\n🎉 \" + \"=\"*80)\n",
    "        print(\"🎉 COMPLETE PIPELINE FINISHED SUCCESSFULLY!\")\n",
    "        print(\"🎉 \" + \"=\"*80)\n",
    "        \n",
    "        # Final summary\n",
    "        if summary and 'test_results' in summary and summary['test_results']:\n",
    "            print(\"\\n📊 FINAL PERFORMANCE SUMMARY:\")\n",
    "            test_results = summary['test_results']\n",
    "            for metric, data in test_results.items():\n",
    "                if isinstance(data, dict) and 'mean' in data:\n",
    "                    print(f\"   {metric.upper()}: {data['mean']:.4f}\")\n",
    "        \n",
    "        print(f\"\\n📁 Check {experiment_dir} for complete results!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏹️  Pipeline stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        print(\"\\n🔧 TROUBLESHOOTING:\")\n",
    "        print(\"1. Check DATA_ROOT path exists and contains image folders\")\n",
    "        print(\"2. Ensure matching image names in input and target folders\")\n",
    "        print(\"3. Try reducing BATCH_SIZE if memory issues\")\n",
    "        print(\"4. Set DEVICE = 'cpu' for GPU issues\")\n",
    "        print(\"5. Check file permissions for OUTPUT_FOLDER\")\n",
    "\n",
    "def create_demo_data(): #UTILITY FUNCTIONS\n",
    "    \"\"\"Create demonstration data with proper folder structure\"\"\"\n",
    "    \n",
    "    print(\"🎨 Creating demonstration data...\")\n",
    "    \n",
    "    demo_root = \"demo_data\"\n",
    "    input_dir = os.path.join(demo_root, \"input\")\n",
    "    target_dir = os.path.join(demo_root, \"target\")\n",
    "    \n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Create diverse sample data\n",
    "    for i in range(100):\n",
    "        # Create input (sketch-like with various styles)\n",
    "        input_array = np.ones((256, 256, 3), dtype=np.uint8) * 255\n",
    "        \n",
    "        # Different sketch styles\n",
    "        if i % 3 == 0:  # Line drawings\n",
    "            for _ in range(np.random.randint(10, 20)):\n",
    "                y = np.random.randint(20, 236)\n",
    "                x_start = np.random.randint(20, 200)\n",
    "                x_end = x_start + np.random.randint(20, 80)\n",
    "                thickness = np.random.randint(1, 3)\n",
    "                input_array[y-thickness:y+thickness, x_start:x_end] = 0\n",
    "        \n",
    "        elif i % 3 == 1:  # Geometric shapes\n",
    "            for _ in range(np.random.randint(3, 8)):\n",
    "                center_x = np.random.randint(50, 206)\n",
    "                center_y = np.random.randint(50, 206)\n",
    "                radius = np.random.randint(10, 30)\n",
    "                \n",
    "                y, x = np.ogrid[:256, :256]\n",
    "                mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "                input_array[mask] = 0\n",
    "        \n",
    "        else:  # Mixed patterns\n",
    "            for _ in range(np.random.randint(15, 25)):\n",
    "                y = np.random.randint(10, 246)\n",
    "                x = np.random.randint(10, 246)\n",
    "                size = np.random.randint(2, 8)\n",
    "                input_array[y:y+size, x:x+size] = 0\n",
    "        \n",
    "        # Create corresponding target (colored version)\n",
    "        target_array = np.random.randint(80, 220, (256, 256, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Add some coherent color regions\n",
    "        for _ in range(np.random.randint(2, 5)):\n",
    "            color = np.random.randint(50, 255, 3)\n",
    "            y_start = np.random.randint(0, 128)\n",
    "            x_start = np.random.randint(0, 128)\n",
    "            y_end = y_start + np.random.randint(64, 128)\n",
    "            x_end = x_start + np.random.randint(64, 128)\n",
    "            target_array[y_start:y_end, x_start:x_end] = color\n",
    "        \n",
    "        # Preserve sketch lines in target\n",
    "        mask = (input_array < 128).any(axis=2)\n",
    "        target_array[mask] = input_array[mask]\n",
    "        \n",
    "        # Save images\n",
    "        input_img = Image.fromarray(input_array)\n",
    "        target_img = Image.fromarray(target_array)\n",
    "        \n",
    "        filename = f\"demo_{i:03d}.png\"\n",
    "        input_img.save(os.path.join(input_dir, filename))\n",
    "        target_img.save(os.path.join(target_dir, filename))\n",
    "    \n",
    "    print(f\"✅ Created 100 demo image pairs in {demo_root}/\")\n",
    "    print(f\"📁 Structure:\")\n",
    "    print(f\"   {demo_root}/\")\n",
    "    print(f\"   ├── input/\")\n",
    "    print(f\"   └── target/\")\n",
    "    \n",
    "    return demo_root\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment to create demo data\n",
    "    # demo_path = create_demo_data()\n",
    "    # print(f\"Demo data created at: {demo_path}\")\n",
    "    # print(\"Update DATA_ROOT to point to this demo data and run again!\")\n",
    "    \n",
    "    # Run complete pipeline\n",
    "    main_complete()\n",
    "\n",
    "\n",
    "DATA_ROOT = \"G:/Jafar/Luna16/\"                    # 🔴 CHANGE THIS: Root folder with input/ and target/ subfolders\n",
    "OUTPUT_FOLDER = \"G:/Jafar/Luna16/Generated/\"      # 🔴 CHANGE THIS: Where to save results\n",
    "\n",
    "IMAGE_SIZE = 256                     # Image resolution (256 or 512)\n",
    "BATCH_SIZE = 16                      # Start with 4, increase if you have good GPU\n",
    "NUM_EPOCHS = 300                     # Number of training epochs\n",
    "LEARNING_RATE = 0.0002               # Learning rate\n",
    "DEVICE = 'auto'                      # 'auto', 'cuda', or 'cpu'\n",
    "\n",
    "TRAIN_RATIO = 0.7                    # 70% for training\n",
    "VAL_RATIO = 0.15                     # 15% for validation\n",
    "TEST_RATIO = 0.15                    # 15% for testing\n",
    "\n",
    "class Config:  #ENHANCED CONFIGURATION CLASS\n",
    "    \"\"\"Configuration class for all settings\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        self.data_root = DATA_ROOT\n",
    "        self.input_folder = os.path.join(DATA_ROOT, \"input\") if os.path.exists(os.path.join(DATA_ROOT, \"input\")) else os.path.join(DATA_ROOT, \"16\")\n",
    "        self.target_folder = os.path.join(DATA_ROOT, \"target\") if os.path.exists(os.path.join(DATA_ROOT, \"target\")) else os.path.join(DATA_ROOT, \"Original\")\n",
    "        self.output_folder = OUTPUT_FOLDER\n",
    "        \n",
    "        # Data split ratios\n",
    "        self.train_ratio = TRAIN_RATIO\n",
    "        self.val_ratio = VAL_RATIO\n",
    "        self.test_ratio = TEST_RATIO\n",
    "        \n",
    "        # Training settings\n",
    "        self.image_size = IMAGE_SIZE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.num_epochs = NUM_EPOCHS\n",
    "        self.learning_rate_g = LEARNING_RATE\n",
    "        self.learning_rate_d = LEARNING_RATE\n",
    "        self.device = self._setup_device(DEVICE)\n",
    "        \n",
    "        # Model settings\n",
    "        self.input_nc = 3\n",
    "        self.output_nc = 3\n",
    "        self.ngf = 64\n",
    "        self.ndf = 64\n",
    "        \n",
    "        # Training parameters\n",
    "        self.beta1 = 0.5\n",
    "        self.beta2 = 0.999\n",
    "        self.lambda_l1 = 100.0\n",
    "        self.lambda_adversarial = 1.0\n",
    "        \n",
    "        # System settings\n",
    "        self.num_workers = 0\n",
    "        self.save_frequency = 10\n",
    "        self.random_seed = 42\n",
    "        \n",
    "        # Create output directory\n",
    "        self.experiment_dir = self._setup_experiment_dir()\n",
    "        \n",
    "        # Validate paths\n",
    "        self._validate_paths()\n",
    "        \n",
    "        logger.info(f\"📁 Input folder: {self.input_folder}\")\n",
    "        logger.info(f\"📁 Target folder: {self.target_folder}\")\n",
    "        logger.info(f\"📁 Results will be saved to: {self.experiment_dir}\")\n",
    "    \n",
    "    def _setup_device(self, device):\n",
    "        \"\"\"Setup device with error handling\"\"\"\n",
    "        if device == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device = 'cuda'\n",
    "                gpu_name = torch.cuda.get_device_name(0)\n",
    "                logger.info(f\"🎮 Using GPU: {gpu_name}\")\n",
    "            else:\n",
    "                device = 'cpu'\n",
    "                logger.info(\"💻 Using CPU (GPU not available)\")\n",
    "        \n",
    "        try:\n",
    "            test_tensor = torch.randn(1, 3, 64, 64).to(device)\n",
    "            logger.info(f\"✅ Device '{device}' ready\")\n",
    "            return device\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ Device '{device}' failed, using CPU: {e}\")\n",
    "            return 'cpu'\n",
    "    \n",
    "    def _validate_paths(self):\n",
    "        \"\"\"Validate and create necessary paths\"\"\"\n",
    "        if not os.path.exists(self.input_folder):\n",
    "            logger.warning(f\"⚠️ Input folder not found: {self.input_folder}\")\n",
    "            os.makedirs(self.input_folder, exist_ok=True)\n",
    "            logger.info(f\"📁 Created input folder: {self.input_folder}\")\n",
    "        \n",
    "        if not os.path.exists(self.target_folder):\n",
    "            logger.warning(f\"⚠️ Target folder not found: {self.target_folder}\")\n",
    "            os.makedirs(self.target_folder, exist_ok=True)\n",
    "            logger.info(f\"📁 Created target folder: {self.target_folder}\")\n",
    "    \n",
    "    def _setup_experiment_dir(self):\n",
    "        \"\"\"Create timestamped experiment directory\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        experiment_dir = os.path.join(self.output_folder, f\"experiment_{timestamp}\")\n",
    "        \n",
    "        # Create subdirectories\n",
    "        subdirs = [\n",
    "            'checkpoints', 'generated_samples', 'training_progress', \n",
    "            'data_splits', 'validation_results', 'test_results', 'final_results'\n",
    "        ]\n",
    "        for subdir in subdirs:\n",
    "            os.makedirs(os.path.join(experiment_dir, subdir), exist_ok=True)\n",
    "        \n",
    "        # Save config\n",
    "        config_path = os.path.join(experiment_dir, 'config.json')\n",
    "        config_dict = {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2, default=str)\n",
    "        \n",
    "        return experiment_dir\n",
    "\n",
    "class DataSplitter:   #DATA SPLITTING AND MANAGEMENT\n",
    "    \"\"\"Handle data splitting and management\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.image_pairs = []\n",
    "        self.train_pairs = []\n",
    "        self.val_pairs = []\n",
    "        self.test_pairs = []\n",
    "        \n",
    "    def find_and_split_data(self):\n",
    "        \"\"\"Find image pairs and split into train/val/test\"\"\"\n",
    "        \n",
    "        logger.info(\"🔍 Scanning for image pairs...\")\n",
    "        self._find_image_pairs()\n",
    "        \n",
    "        if len(self.image_pairs) == 0:\n",
    "            logger.warning(\"⚠️ No valid image pairs found! Creating sample data...\")\n",
    "            self._create_sample_data()\n",
    "            self._find_image_pairs()\n",
    "        \n",
    "        logger.info(f\"📊 Found {len(self.image_pairs)} total image pairs\")\n",
    "        \n",
    "        # Split data\n",
    "        self._split_data()\n",
    "        \n",
    "        # Save split information\n",
    "        self._save_split_info()\n",
    "        \n",
    "        logger.info(f\"✅ Data split completed:\")\n",
    "        logger.info(f\"   📚 Training: {len(self.train_pairs)} pairs\")\n",
    "        logger.info(f\"   🔍 Validation: {len(self.val_pairs)} pairs\")\n",
    "        logger.info(f\"   🧪 Testing: {len(self.test_pairs)} pairs\")\n",
    "        \n",
    "        return self.train_pairs, self.val_pairs, self.test_pairs\n",
    "    \n",
    "    def _find_image_pairs(self):\n",
    "        \"\"\"Find matching input-target image pairs\"\"\"\n",
    "        extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']\n",
    "        \n",
    "        # Get input files\n",
    "        input_files = []\n",
    "        for ext in extensions:\n",
    "            input_files.extend(glob.glob(os.path.join(self.config.input_folder, ext)))\n",
    "            input_files.extend(glob.glob(os.path.join(self.config.input_folder, ext.upper())))\n",
    "        \n",
    "        logger.info(f\"📸 Found {len(input_files)} input images\")\n",
    "        \n",
    "        # Find matching targets and validate\n",
    "        valid_pairs = []\n",
    "        for input_path in tqdm(input_files, desc=\"Validating image pairs\"):\n",
    "            input_name = Path(input_path).stem\n",
    "            \n",
    "            # Find matching target\n",
    "            target_path = None\n",
    "            for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
    "                for target_ext in [ext, ext.upper()]:\n",
    "                    candidate = os.path.join(self.config.target_folder, input_name + target_ext)\n",
    "                    if os.path.exists(candidate):\n",
    "                        target_path = candidate\n",
    "                        break\n",
    "                if target_path:\n",
    "                    break\n",
    "            \n",
    "            if target_path and self._validate_image_pair(input_path, target_path):\n",
    "                valid_pairs.append((input_path, target_path))\n",
    "        \n",
    "        self.image_pairs = valid_pairs\n",
    "    \n",
    "    def _validate_image_pair(self, input_path, target_path):\n",
    "        \"\"\"Validate that an image pair can be loaded\"\"\"\n",
    "        try:\n",
    "            input_img = Image.open(input_path).convert('RGB')\n",
    "            target_img = Image.open(target_path).convert('RGB')\n",
    "            \n",
    "            # Test resizing\n",
    "            input_img.resize((64, 64), Image.LANCZOS)\n",
    "            target_img.resize((64, 64), Image.LANCZOS)\n",
    "            \n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _split_data(self):\n",
    "        \"\"\"Split data into train/validation/test sets\"\"\"\n",
    "        \n",
    "        # Set random seed for reproducible splits\n",
    "        random.seed(self.config.random_seed)\n",
    "        np.random.seed(self.config.random_seed)\n",
    "        \n",
    "        # Shuffle the pairs\n",
    "        shuffled_pairs = self.image_pairs.copy()\n",
    "        random.shuffle(shuffled_pairs)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        total_pairs = len(shuffled_pairs)\n",
    "        train_size = int(total_pairs * self.config.train_ratio)\n",
    "        val_size = int(total_pairs * self.config.val_ratio)\n",
    "        \n",
    "        # Split the data\n",
    "        self.train_pairs = shuffled_pairs[:train_size]\n",
    "        self.val_pairs = shuffled_pairs[train_size:train_size + val_size]\n",
    "        self.test_pairs = shuffled_pairs[train_size + val_size:]\n",
    "        \n",
    "        # Ensure we have data in each split\n",
    "        if len(self.test_pairs) == 0 and len(self.val_pairs) > 1:\n",
    "            # Move one from val to test\n",
    "            self.test_pairs.append(self.val_pairs.pop())\n",
    "        \n",
    "        if len(self.val_pairs) == 0 and len(self.train_pairs) > 1:\n",
    "            # Move one from train to val\n",
    "            self.val_pairs.append(self.train_pairs.pop())\n",
    "    \n",
    "    def _save_split_info(self):\n",
    "        \"\"\"Save information about the data splits\"\"\"\n",
    "        split_info = {\n",
    "            'total_pairs': len(self.image_pairs),\n",
    "            'train_pairs': len(self.train_pairs),\n",
    "            'val_pairs': len(self.val_pairs),\n",
    "            'test_pairs': len(self.test_pairs),\n",
    "            'train_ratio': self.config.train_ratio,\n",
    "            'val_ratio': self.config.val_ratio,\n",
    "            'test_ratio': self.config.test_ratio,\n",
    "            'random_seed': self.config.random_seed,\n",
    "            'split_timestamp': datetime.now().isoformat(),\n",
    "            'train_files': [(str(i), str(t)) for i, t in self.train_pairs],\n",
    "            'val_files': [(str(i), str(t)) for i, t in self.val_pairs],\n",
    "            'test_files': [(str(i), str(t)) for i, t in self.test_pairs]\n",
    "        }\n",
    "        \n",
    "        split_path = os.path.join(self.config.experiment_dir, 'data_splits', 'split_info.json')\n",
    "        with open(split_path, 'w') as f:\n",
    "            json.dump(split_info, f, indent=2)\n",
    "        \n",
    "        # Create CSV files for easy viewing\n",
    "        splits = [\n",
    "            ('train', self.train_pairs),\n",
    "            ('val', self.val_pairs),\n",
    "            ('test', self.test_pairs)\n",
    "        ]\n",
    "        \n",
    "        for split_name, pairs in splits:\n",
    "            df_data = []\n",
    "            for input_path, target_path in pairs:\n",
    "                df_data.append({\n",
    "                    'input_path': input_path,\n",
    "                    'target_path': target_path,\n",
    "                    'filename': Path(input_path).name\n",
    "                })\n",
    "            \n",
    "            if df_data:\n",
    "                df = pd.DataFrame(df_data)\n",
    "                csv_path = os.path.join(self.config.experiment_dir, 'data_splits', f'{split_name}_files.csv')\n",
    "                df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    def _create_sample_data(self):\n",
    "        \"\"\"Create sample data if no valid data found\"\"\"\n",
    "        logger.info(\"🎨 Creating sample data for demonstration...\")\n",
    "        \n",
    "        num_samples = 50\n",
    "        for i in range(num_samples):\n",
    "            # Create input image (sketch-like)\n",
    "            input_array = np.ones((256, 256, 3), dtype=np.uint8) * 255\n",
    "            for _ in range(15):\n",
    "                y = np.random.randint(20, 236)\n",
    "                x_start = np.random.randint(20, 200)\n",
    "                x_end = x_start + np.random.randint(20, 60)\n",
    "                thickness = np.random.randint(1, 4)\n",
    "                input_array[y-thickness:y+thickness, x_start:x_end] = 0\n",
    "            \n",
    "            # Create target image (colored)\n",
    "            target_array = np.random.randint(50, 200, (256, 256, 3), dtype=np.uint8)\n",
    "            mask = (input_array < 128).any(axis=2)\n",
    "            target_array[mask] = input_array[mask]\n",
    "            \n",
    "            # Save images\n",
    "            input_img = Image.fromarray(input_array)\n",
    "            target_img = Image.fromarray(target_array)\n",
    "            \n",
    "            filename = f\"sample_{i:03d}.png\"\n",
    "            input_img.save(os.path.join(self.config.input_folder, filename))\n",
    "            target_img.save(os.path.join(self.config.target_folder, filename))\n",
    "        \n",
    "        logger.info(f\"✅ Created {num_samples} sample image pairs\")\n",
    "\n",
    "class Pix2PixDataset(Dataset):  ENHANCED DATASET WITH SPLIT SUPPORT\n",
    "    \"\"\"Dataset class supporting train/val/test splits\"\"\"\n",
    "    \n",
    "    def __init__(self, image_pairs, config, split_type='train'):\n",
    "        self.image_pairs = image_pairs\n",
    "        self.config = config\n",
    "        self.split_type = split_type\n",
    "        \n",
    "        # Data augmentation for training\n",
    "        if split_type == 'train':\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((config.image_size, config.image_size), Image.LANCZOS),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x * 2.0 - 1.0)  # Normalize to [-1, 1]\n",
    "            ])\n",
    "        else:\n",
    "            # No augmentation for validation/test\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((config.image_size, config.image_size), Image.LANCZOS),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x * 2.0 - 1.0)  # Normalize to [-1, 1]\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            input_path, target_path = self.image_pairs[idx]\n",
    "            \n",
    "            # Load images\n",
    "            input_img = Image.open(input_path).convert('RGB')\n",
    "            target_img = Image.open(target_path).convert('RGB')\n",
    "            \n",
    "            # Apply same random transformations to both images\n",
    "            if self.split_type == 'train':\n",
    "                # Apply same random seed for synchronized augmentation\n",
    "                seed = np.random.randint(2147483647)\n",
    "                \n",
    "                random.seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                input_tensor = self.transform(input_img)\n",
    "                \n",
    "                random.seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                target_tensor = self.transform(target_img)\n",
    "            else:\n",
    "                input_tensor = self.transform(input_img)\n",
    "                target_tensor = self.transform(target_img)\n",
    "            \n",
    "            return {\n",
    "                'input': input_tensor,\n",
    "                'target': target_tensor,\n",
    "                'input_path': input_path,\n",
    "                'target_path': target_path,\n",
    "                'filename': Path(input_path).stem\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading item {idx}: {e}\")\n",
    "            # Return dummy data\n",
    "            dummy = torch.zeros(3, self.config.image_size, self.config.image_size)\n",
    "            return {\n",
    "                'input': dummy,\n",
    "                'target': dummy,\n",
    "                'input_path': '',\n",
    "                'target_path': '',\n",
    "                'filename': 'error'\n",
    "            }\n",
    "\n",
    "\n",
    "class EnhancedGenerator(nn.Module):  #ENHANCED MODELS (UNCHANGED FROM ORIGINAL)\n",
    "    \"\"\"Enhanced U-Net Generator\"\"\"\n",
    "    \n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.e1 = self._make_layer(input_nc, ngf, normalize=False)\n",
    "        self.e2 = self._make_layer(ngf, ngf * 2)\n",
    "        self.e3 = self._make_layer(ngf * 2, ngf * 4)\n",
    "        self.e4 = self._make_layer(ngf * 4, ngf * 8)\n",
    "        self.e5 = self._make_layer(ngf * 8, ngf * 8)\n",
    "        self.e6 = self._make_layer(ngf * 8, ngf * 8)\n",
    "        self.e7 = self._make_layer(ngf * 8, ngf * 8)\n",
    "        self.e8 = self._make_layer(ngf * 8, ngf * 8, normalize=False)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.d1 = self._make_up_layer(ngf * 8, ngf * 8, dropout=True)\n",
    "        self.d2 = self._make_up_layer(ngf * 16, ngf * 8, dropout=True)\n",
    "        self.d3 = self._make_up_layer(ngf * 16, ngf * 8, dropout=True)\n",
    "        self.d4 = self._make_up_layer(ngf * 16, ngf * 8)\n",
    "        self.d5 = self._make_up_layer(ngf * 16, ngf * 4)\n",
    "        self.d6 = self._make_up_layer(ngf * 8, ngf * 2)\n",
    "        self.d7 = self._make_up_layer(ngf * 4, ngf)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 2, output_nc, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, normalize=True):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_up_layer(self, in_channels, out_channels, dropout=False):\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d(0.5))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.e1(x)\n",
    "        e2 = self.e2(e1)\n",
    "        e3 = self.e3(e2)\n",
    "        e4 = self.e4(e3)\n",
    "        e5 = self.e5(e4)\n",
    "        e6 = self.e6(e5)\n",
    "        e7 = self.e7(e6)\n",
    "        e8 = self.e8(e7)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d1 = self.d1(e8)\n",
    "        d2 = self.d2(torch.cat([d1, e7], 1))\n",
    "        d3 = self.d3(torch.cat([d2, e6], 1))\n",
    "        d4 = self.d4(torch.cat([d3, e5], 1))\n",
    "        d5 = self.d5(torch.cat([d4, e4], 1))\n",
    "        d6 = self.d6(torch.cat([d5, e3], 1))\n",
    "        d7 = self.d7(torch.cat([d6, e2], 1))\n",
    "        \n",
    "        output = self.final(torch.cat([d7, e1], 1))\n",
    "        return output\n",
    "\n",
    "class EnhancedDiscriminator(nn.Module):\n",
    "    \"\"\"Enhanced PatchGAN Discriminator\"\"\"\n",
    "    \n",
    "    def __init__(self, input_nc=6, ndf=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, ndf, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 1, 1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 1)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, input_img, target_img):\n",
    "        x = torch.cat([input_img, target_img], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class EnhancedMetrics:  #COMPREHENSIVE METRICS CALCULATOR\n",
    "    \"\"\"Comprehensive metrics calculator for image quality assessment\"\"\"\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "    \n",
    "    def calculate_psnr(self, img1, img2, max_val=1.0):\n",
    "        \"\"\"Calculate Peak Signal-to-Noise Ratio\"\"\"\n",
    "        mse = torch.mean((img1 - img2) ** 2)\n",
    "        if mse == 0:\n",
    "            return float('inf')\n",
    "        return 20 * torch.log10(max_val / torch.sqrt(mse))\n",
    "    \n",
    "    def calculate_ssim(self, img1, img2):\n",
    "        \"\"\"Calculate Structural Similarity Index\"\"\"\n",
    "        img1_np = img1.detach().cpu().numpy()\n",
    "        img2_np = img2.detach().cpu().numpy()\n",
    "        \n",
    "        ssim_values = []\n",
    "        for i in range(img1_np.shape[0]):\n",
    "            im1 = np.transpose(img1_np[i], (1, 2, 0))\n",
    "            im2 = np.transpose(img2_np[i], (1, 2, 0))\n",
    "            \n",
    "            # Ensure values are in [0, 1] range\n",
    "            im1 = np.clip((im1 + 1) / 2, 0, 1)\n",
    "            im2 = np.clip((im2 + 1) / 2, 0, 1)\n",
    "            \n",
    "            # Calculate SSIM\n",
    "            ssim_val = ssim(im1, im2, multichannel=True, data_range=1.0, channel_axis=2)\n",
    "            ssim_values.append(ssim_val)\n",
    "        \n",
    "        return np.mean(ssim_values)\n",
    "    \n",
    "    def calculate_lpips(self, img1, img2):\n",
    "        \"\"\"Calculate simplified LPIPS-like perceptual metric\"\"\"\n",
    "        img1_gray = torch.mean(img1, dim=1, keepdim=True)\n",
    "        img2_gray = torch.mean(img2, dim=1, keepdim=True)\n",
    "        \n",
    "        # Sobel filters\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                              dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                              dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        grad1_x = F.conv2d(img1_gray, sobel_x, padding=1)\n",
    "        grad1_y = F.conv2d(img1_gray, sobel_y, padding=1)\n",
    "        grad2_x = F.conv2d(img2_gray, sobel_x, padding=1)\n",
    "        grad2_y = F.conv2d(img2_gray, sobel_y, padding=1)\n",
    "        \n",
    "        # Perceptual difference\n",
    "        diff = torch.mean(torch.abs(grad1_x - grad2_x) + torch.abs(grad1_y - grad2_y))\n",
    "        return diff.item()\n",
    "    \n",
    "    def evaluate_comprehensive(self, generated, target):\n",
    "        \"\"\"Calculate all metrics for a batch\"\"\"\n",
    "        with torch.no_grad():\n",
    "            metrics = {}\n",
    "            \n",
    "            # Basic metrics\n",
    "            metrics['psnr'] = self.calculate_psnr(generated, target).item()\n",
    "            metrics['mae'] = F.l1_loss(generated, target).item()\n",
    "            metrics['mse'] = F.mse_loss(generated, target).item()\n",
    "            \n",
    "            # Advanced metrics\n",
    "            metrics['ssim'] = self.calculate_ssim(generated, target)\n",
    "            metrics['lpips'] = self.calculate_lpips(generated, target)\n",
    "            \n",
    "            # Edge preservation metric\n",
    "            generated_edges = self._detect_edges(generated)\n",
    "            target_edges = self._detect_edges(target)\n",
    "            metrics['edge_similarity'] = F.cosine_similarity(\n",
    "                generated_edges.flatten(1), target_edges.flatten(1)\n",
    "            ).mean().item()\n",
    "            \n",
    "            return metrics\n",
    "    \n",
    "    def _detect_edges(self, images):\n",
    "        \"\"\"Detect edges using Sobel operator\"\"\"\n",
    "        gray = torch.mean(images, dim=1, keepdim=True)\n",
    "        \n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                              dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                              dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        grad_x = F.conv2d(gray, sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(gray, sobel_y, padding=1)\n",
    "        \n",
    "        edges = torch.sqrt(grad_x**2 + grad_y**2)\n",
    "        return edges\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
